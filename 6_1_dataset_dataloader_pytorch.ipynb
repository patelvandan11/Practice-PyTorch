{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent and Data Management with PyTorch\n",
    "\n",
    "#### Theory\n",
    "\n",
    "Mini-batch gradient descent is an optimization algorithm used in training machine learning models. It combines the advantages of both batch gradient descent and stochastic gradient descent by dividing the training data into smaller batches. Each batch is used to compute the gradient and update the model parameters.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Batch Gradient Descent:**\n",
    "   - Uses the entire dataset to calculate the gradient.\n",
    "   - Pros: Stable convergence.\n",
    "   - Cons: Memory-intensive and slow parameter updates.\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD):**\n",
    "   - Uses a single data point to calculate the gradient.\n",
    "   - Pros: Faster updates.\n",
    "   - Cons: Noisy convergence.\n",
    "\n",
    "3. **Mini-batch Gradient Descent:**\n",
    "   - Splits the dataset into smaller batches for gradient calculation.\n",
    "   - Pros:\n",
    "     - Efficient memory usage.\n",
    "     - Faster convergence than batch gradient descent.\n",
    "     - Reduces noise compared to SGD.\n",
    "\n",
    "#### Problems with Mini-batch Gradient Descent\n",
    "\n",
    "1. No standard interface for data handling (e.g., image classification datasets stored in different folders).\n",
    "   - **Example:** In image classification, raw datasets may be organized differently across projects, such as images stored in class-specific directories or requiring specific preprocessing pipelines.\n",
    "\n",
    "2. Difficulty in applying transformations (e.g., data augmentation).\n",
    "   - **Example:** Applying transformations like cropping, flipping, or normalizing images may require manual implementation for every dataset.\n",
    "\n",
    "3. Shuffling and sampling issues.\n",
    "   - **Example:** Randomly shuffling large datasets to ensure generalization can be challenging without automated tools.\n",
    "\n",
    "4. Managing batches and parallelization.\n",
    "   - **Example:** Creating efficient batches while utilizing multiple cores for parallel data loading can be complex and error-prone.\n",
    "\n",
    "\n",
    "#### Solution: PyTorch `Dataset` and `DataLoader`\n",
    "\n",
    "PyTorch provides two essential classes to address these issues:\n",
    "\n",
    "1. **`Dataset`:**\n",
    "   - A standard interface for accessing and transforming data.\n",
    "   - Custom datasets can be created by subclassing `torch.utils.data.Dataset`.\n",
    "\n",
    "2. **`DataLoader`:**\n",
    "   - Handles batching, shuffling, and parallelized data loading.\n",
    "   - Simplifies the process of preparing data for training.\n",
    "\n",
    "By using `Dataset` and `DataLoader`, we can:\n",
    "- Apply transformations easily.\n",
    "- Handle large datasets efficiently.\n",
    "- Shuffle and sample data for better generalization.\n",
    "- Parallelize data loading to speed up training.\n",
    "\n",
    "Below is a practical implementation of these concepts:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader in PyTorch\n",
    "\n",
    "PyTorch provides two core abstractions that **decouple how you define your data** from **how you efficiently iterate over it in training loops**:\n",
    "\n",
    "---\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "- **Dataset Class**: \n",
    "  The Dataset class acts as a blueprint(abstract class). When creating a custom Dataset, you decide how the data is loaded and returned. It defines:\n",
    "  - `__init__()`: Specifies how data should be loaded.</span>\n",
    "  - `__len__()` : Returns the total number of samples in the dataset.(batch_size)\n",
    "  - `__getitem__(index)` : Fetches data (and labels) for a given index.\n",
    "\n",
    "---\n",
    "\n",
    "### DataLoader Class\n",
    "\n",
    "- The DataLoader wraps a Dataset and:\n",
    "  - Handles **batching**.\n",
    "  - Manages **shuffling**.\n",
    "  - Enables **parallelized data loading** for faster processing.\n",
    "\n",
    "---\n",
    "\n",
    "### DataLoader Control Flow\n",
    "\n",
    "1. At the start of each epoch, the DataLoader (with `shuffle=True`) shuffles indices. \n",
    "- Example:  \n",
    "     ```\n",
    "     Initial indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
    "     Shuffled indices: [4, 5, 8, 9, 7, 2, 6, 1, 3, 0]\n",
    "     ```\n",
    "2. It divides these indices into chunks of the specified `batch_size`. \n",
    "- For example, with `batch_size=2`:  \n",
    "     ```\n",
    "     [[4, 5], [8, 9], [7, 2], [6, 1], [3, 0]]\n",
    "     ```\n",
    "3. For each index in the chunk, data samples are fetched from the Dataset object.\n",
    "- Example:  \n",
    "     - Chunk: `[4, 5]`  \n",
    "     - `Dataset.__getitem__(4)` retrieves the data for index 4.  \n",
    "     - `Dataset.__getitem__(5)` retrieves the data for index 5.  \n",
    "\n",
    "\n",
    "4. The samples are then collected and combined into a batch (using `collate_fn`)\n",
    "- This process uses the `collate_fn` function, which determines how individual samples are grouped together.  \n",
    "   - Example:  \n",
    "     ```\n",
    "     Batch: [Sample_4, Sample_5]\n",
    "     ```\n",
    "\n",
    "5. Returning the Batch  \n",
    "   - The batch is then returned to the main training loop for processing.  \n",
    "   - This process is repeated until all chunks (batches) in the epoch are consumed.  \n",
    "\n",
    "6. Iterating Over Epochs \n",
    "   - Once all batches have been processed, the process repeats for the next epoch, starting again with shuffling the indices.  \n",
    "   - Example Workflow for an Epoch:  \n",
    "     ```\n",
    "     Epoch Start -> Shuffle Indices -> Divide into Batches -> Fetch Samples -> Collate Samples -> Return Batch\n",
    "     ```\n",
    "\n",
    "\n",
    "* Shuffling Indices  \n",
    "* Dividing into Chunks  \n",
    "* Fetching Data Samples  \n",
    "* Combining Samples into a Batch  \n",
    "* Returning the Batch  \n",
    "* Iterating Over Epochs  \n",
    "\n",
    "\n",
    "---\n",
    "#### Flow of Data Management\n",
    "\n",
    "```plaintext\n",
    "The Dataset (memory)        \n",
    "    |\n",
    "    |\n",
    "    V\n",
    "Dataset Class (defines where data is stored and fetches data one by one)    \n",
    "    |\n",
    "    |\n",
    "    v\n",
    "DataLoader Class (manages batching and defines number of rows per batch)\n",
    "```\n",
    "\n",
    "By using `Dataset` and `DataLoader`, we can:\n",
    "- Apply transformations easily.\n",
    "- Handle large datasets efficiently.\n",
    "- Shuffle and sample data for better generalization.\n",
    "- Parallelize data loading to speed up training.\n",
    "\n",
    "Below is a practical implementation of these concepts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.datasets import  make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=make_classification(\n",
    "    n_samples=10,   # number of samples\n",
    "    n_features=2,   # number of features\n",
    "    n_informative=2,    # number of informative features\n",
    "    n_redundant=0, # number of redundant features\n",
    "    n_classes=2,   # number of classes\n",
    "    random_state=42  # random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.06833894, -0.97007347],\n",
       "       [-1.14021544, -0.83879234],\n",
       "       [-2.8953973 ,  1.97686236],\n",
       "       [-0.72063436, -0.96059253],\n",
       "       [-1.96287438, -0.99225135],\n",
       "       [-0.9382051 , -0.54304815],\n",
       "       [ 1.72725924, -1.18582677],\n",
       "       [ 1.77736657,  1.51157598],\n",
       "       [ 1.89969252,  0.83444483],\n",
       "       [-0.58723065, -1.97171753]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  convert the data to pytorch tensor \n",
    "x=torch.tensor(x,dtype=torch.float32)\n",
    "y=torch.tensor(y,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0683, -0.9701],\n",
       "        [-1.1402, -0.8388],\n",
       "        [-2.8954,  1.9769],\n",
       "        [-0.7206, -0.9606],\n",
       "        [-1.9629, -0.9923],\n",
       "        [-0.9382, -0.5430],\n",
       "        [ 1.7273, -1.1858],\n",
       "        [ 1.7774,  1.5116],\n",
       "        [ 1.8997,  0.8344],\n",
       "        [-0.5872, -1.9717]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customdataset(Dataset):\n",
    "    def __init__(self,features,labels):\n",
    "        self.features=features\n",
    "        self.labels=labels\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.features[idx],self.labels[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=Customdataset(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.0683, -0.9701]), tensor(1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=DataLoader(dataset,batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9382, -0.5430],\n",
      "        [-0.5872, -1.9717]])\n",
      "tensor([1, 0])\n",
      "--------------------------------------------------\n",
      "tensor([[ 1.7273, -1.1858],\n",
      "        [-2.8954,  1.9769]])\n",
      "tensor([1, 0])\n",
      "--------------------------------------------------\n",
      "tensor([[-1.9629, -0.9923],\n",
      "        [ 1.7774,  1.5116]])\n",
      "tensor([0, 1])\n",
      "--------------------------------------------------\n",
      "tensor([[-1.1402, -0.8388],\n",
      "        [-0.7206, -0.9606]])\n",
      "tensor([0, 0])\n",
      "--------------------------------------------------\n",
      "tensor([[ 1.0683, -0.9701],\n",
      "        [ 1.8997,  0.8344]])\n",
      "tensor([1, 1])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for batch_feature,batch_label in dataloader:\n",
    "    print(batch_feature)\n",
    "    print(batch_label)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

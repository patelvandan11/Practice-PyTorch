{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The NN module\n",
    "* The `torch.nn` module in PyTorch is a core library that provides:\n",
    "    - Pre-built layers\n",
    "    - Loss functions\n",
    "    - Activation functions\n",
    "    - Other utilities\n",
    "\n",
    "* It abstracts the complexity of creating and training neural networks, enabling you to focus on designing and experimenting with model architectures.\n",
    "---\n",
    "Key Components of torch.nn:\n",
    "---\n",
    "- **Modules (Layers)**:\n",
    "    - `nn.Module`: The base class for all neural network modules. Your custom models and layers should subclass this class.\n",
    "    - Common Layers: Includes layers like `nn.Linear` (fully connected layer), `nn.Conv2d` (convolutional layer), `nn.LSTM` (recurrent layer), and many others.\n",
    "- **Activation Functions**:\n",
    "    - Functions like `nn.ReLU`, `nn.Sigmoid`, and `nn.Tanh` introduce non-linearities to the model, allowing it to learn complex patterns.\n",
    "- **Loss Functions**:\n",
    "    - Provides loss functions such as `nn.CrossEntropyLoss`, `nn.MSELoss`, and `nn.NLLLoss` to quantify the difference between the model's predictions and the actual targets.\n",
    "- **Container Modules**:\n",
    "    - `nn.Sequential`: A sequential container to stack layers in order.\n",
    "- **Regularization and Dropout**:\n",
    "    - Layers like `nn.Dropout` and `nn.BatchNorm2d` help prevent overfitting and improve the model's ability to generalize to new data.Key Components of torch.nn:\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.simple NN model perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_feature):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_feature, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,feature):\n",
    "        out=self.linear(feature)\n",
    "        out=self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4541],\n",
       "        [0.4923],\n",
       "        [0.5502],\n",
       "        [0.4923],\n",
       "        [0.4979],\n",
       "        [0.5361],\n",
       "        [0.5399],\n",
       "        [0.4384],\n",
       "        [0.5263],\n",
       "        [0.4536]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a data set \n",
    "feature=torch.rand(10,5)\n",
    "# create a model\n",
    "model=MyModel(feature.shape[1])\n",
    "# call model for forward pass\n",
    "model(feature)  # this will call forward function because of __call__ function in nn.Module class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4217, -0.3088, -0.1235, -0.3255,  0.4284]], requires_grad=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  show model weights\n",
    "model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0061], requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyModel                                  [10, 1]                   --\n",
       "├─Linear: 1-1                            [10, 1]                   6\n",
       "├─Sigmoid: 1-2                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 6\n",
       "Trainable params: 6\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.NN model \n",
    "1. 5 input node,\n",
    "2. 3 hidden node-relu,\n",
    "3. 1 output node-sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_feature):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_feature, 3)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2 = nn.Linear(3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x=self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5158],\n",
       "        [0.5198],\n",
       "        [0.4929],\n",
       "        [0.4855],\n",
       "        [0.4647],\n",
       "        [0.4697],\n",
       "        [0.4476],\n",
       "        [0.4662],\n",
       "        [0.4647],\n",
       "        [0.4647]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a data set \n",
    "feature=torch.rand(10,5)\n",
    "# create a model\n",
    "M=Model(feature.shape[1])\n",
    "# call model for forward pass\n",
    "M(feature)  # this will call forward function because of __call__ function in nn.Module class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3964,  0.1857,  0.1789, -0.3187, -0.1423],\n",
       "        [ 0.2935, -0.2449, -0.3288,  0.3886, -0.2425],\n",
       "        [-0.3004, -0.0217, -0.0841, -0.2912, -0.0376]], requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.linear1.weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5638, -0.1067, -0.4889]], requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.linear2.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1416], requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.linear2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0577, -0.1911,  0.3462], requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.linear1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [10, 1]                   --\n",
       "├─Linear: 1-1                            [10, 3]                   18\n",
       "├─ReLU: 1-2                              [10, 3]                   --\n",
       "├─Linear: 1-3                            [10, 1]                   4\n",
       "├─Sigmoid: 1-4                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 22\n",
       "Trainable params: 22\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(M, input_size=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.use a squential model to build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_feature):\n",
    "        super().__init__()\n",
    "        self.Network = nn.Sequential(\n",
    "            nn.Linear(num_feature, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x=self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  imporve the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
